\documentclass[9pt, a4paper]{extarticle}

\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[T1]{fontenc}
\usepackage{lmodern}

\usepackage{multicol}
\usepackage{titlesec}

\setlength{\parindent}{0pt}
\usepackage{enumitem}
\usepackage[a4paper, left=0cm, right=0cm, top=0cm, bottom=0.2cm, landscape=true]{geometry}
\allowdisplaybreaks
\titlespacing*{\section}{0pt}{0pt}{0pt}
\titlespacing*{\subsection}{0pt}{0.5pt}{0.6pt}
\titlespacing*{\subsubsection}{0pt}{0.5pt}{0.6pt}

\DeclareMathOperator{\decode}{decode}
\DeclareMathOperator{\code}{code}
\DeclareMathOperator{\loope}{LOOP}
\DeclareMathOperator{\var}{var}
\DeclareMathOperator{\ife}{IF}
\DeclareMathOperator{\elsee}{ELSE}
\DeclareMathOperator{\while}{WHILE}
\DeclareMathOperator{\dive}{DIV}
\DeclareMathOperator{\pr}{pr}
\DeclareMathOperator{\undef}{undef}
\DeclareMathOperator{\nf}{nf}
\DeclareMathOperator{\NP}{NP}
\DeclareMathOperator{\np}{np}
\DeclareMathOperator{\mult}{mult}
\DeclareMathOperator{\add}{add}
\DeclareMathOperator{\sub}{sub}
\DeclareMathOperator{\vg}{vg}

\begin{document}
\setlist{nolistsep}

\begin{multicols}{5}
    \section{Maschine Learning} Algorithm learns class of tasks, measured by loss function, from experience.

    \textbf{supervised learning} learn $h: \Delta^* \to \Sigma^*, h=t$; example: $(x,y)\in\Delta^*\times\Sigma^*,\ t(x)=y$.

    \textbf{unsupervised learning} learn $h: \Delta^* \to \Sigma^*, \ker(h)=\ker(t)$; example: $x\in\Delta^*$.

    \textbf{reinforcement learning} learn strategy based on feedback from environment.
    \section{Supervised Learning}
    - model function $t: \mathcal{M} \to \mathcal{R}$

    - $supp(t)=\{m\in \mathcal{M} \mid t(m) \neq 0\}$

    - $\bar{m} \in supp(t) \Leftrightarrow t(\bar{m})=1$

    \textbf{Hypothesis} of A: potential result of A

    \textbf{Hypothesis space} $\mathcal{H}_A$ of A: set of all hypotheses

    \textbf{h fits D} if $h(x_i)=y_i$ for all $(x_i,y_i)\in D$

    \textbf{Version space} $\mathcal{V}_A(D)$ of A: all hypotheses that fit D

    \textbf{Inductive bias} of A: set of assumptions that A uses to predict outputs of unseen data

    \subsection{Conjunctive Clause}

    $\theta=(\theta_1, ..., \theta_k), \theta_i \in M_i\cup \{\star, \bot\}  $

    - $\theta_\bot = (\bot, ..., \bot)$ most specific

    - $\theta_\star = (\star, ..., \star)$ most general

    - $supp(h_{\theta_\bot})=\emptyset, supp(h_{\theta_\star})=\mathcal{M}$

    - $h_{\theta_\bot}=h_{(\theta_1, ..., \bot, ..., \theta_k) = ...}$

    induced hypothesis $h_\theta (m_1, ..., m_k)=1$ if $\forall i: \theta_i \in
        \{m_i, \star\}$ else $0$

    $h \preceq h'$ if $supp(h) \subseteq supp(h')$. $h$ is more specfic (less general) than $h'$

    \textbf{Find-S Algorithm} finds most specific conjunctive clause that fits D
    \begin{enumerate}
        \item Start with $\theta_\bot=(\bot, ..., \bot)$
        \item iterate over POSITIVE examples
        \item min-generalize $\theta$ to fit example
        \item $\bot \to a, a \to \star$
    \end{enumerate}
    - maximal general hypothsis:
    \begin{enumerate}
        \item start at $\theta_\star = (\star, ..., \star)$
        \item exclude every negative example
        \item $(\star, ...) \to \{(b, ...), (c, ...)...   \}$
    \end{enumerate}

    - If $V_A(D) \neq \emptyset$, Find-S finds $h\in V_A(D)$

    \textbf{disjunctive normal form} $\Theta = \{\theta_1, ..., \theta_m\}$

    'finite set of conjunctive clauses'

    induced hypothesis $h_\Theta (\bar{m})=1$ if $\exists \theta \in \Theta:
        h_\theta(\bar{m})=1$ else $0$

    - $supp(\Theta) = \bigcup_{\theta \in \Theta} supp(h_\theta)$

    - can represent all boolean functions
    \par
    \columnbreak
    \textbf{Boundary sets of version space}

    maximally general hypotheses $V_A^\top(D) =$

    $ \{h \in V_A(D) \mid \nexists h' \in V_A(D): h \prec h'\}$

    maximally specific hypotheses $V_A^\bot(D)=$

    $\{ h \in V_A(D) \mid \nexists h' \in V_A(D): h' \preceq h\}$

    -$h\in V_A^\top$ maximal, weil: $\forall x \in M\setminus supp(h): supp(h)\cup \{x \} \notin supp(V_A(D))$

    Theorem: $V_A(D)=\{h \in H_A | \exists h_\top \in V_A^\top(D), \exists h_\bot
        \in V_A^\bot(D): h_\bot \preceq h \preceq h_\top\}$

    -> $V_A(D)$ det. by $V_A^\top(D)$ and $V_A^\bot(D)$

    - only 1 lower bound (in $V_A^\bot(D)$), potentially multiple upper bounds (in $V_A^\top(D)$)

    \textbf{Candidate Elimination Algorithm}

    Output: DNF for $V_A^\top(D)$ and $V_A^\bot(D)$

    1. $S_\bot=\{\theta_\bot\}, S_\top=\{\theta_\star\}$

    2. for $1\leq i \leq n: y_i=1$ (pos. xmpls)
    \begin{enumerate}
        \item keep only fitting h from $S_\top$
        \item $\forall \theta \in S_\bot: h_\theta(x_i)=0$

              - remove $\theta$, add all min generalizations $\theta'$ of $\theta$ that fit $x_i$ to $S_\bot$
        \item keep only most specific h in $S_\bot$
    \end{enumerate}
    3. for $1\leq i \leq n: y_i=0$ (neg. xmpls)
    \begin{enumerate}
        \item keep only fitting h from $S_\bot$
        \item $\forall \theta \in S_\top: h_\theta(x_i)=1$

              - remove $\theta$, add all min specializations $\theta'$ of $\theta$ that fit $x_i$ to $S_\top$,

              for which a more specific $\theta_\bot \in S_\bot$ exists!
        \item keep only most general h in $S_\top$

    \end{enumerate}
    - $V_A^\top=\{h_\theta | \theta \in S_\top\}$, $V_A^\bot=\{h_\theta | \theta \in S_\bot\}$

    - Concept indentified if: $S_\bot = S_\top$ and $|S_\top|=1$. $V_A(D)=\emptyset$ if $S_\bot =\emptyset \vee S_\top = \emptyset$

    \subsection{Decision Trees}
    \textbf{Splitting } $\Pi=\{M-1, ..., M_p\} $ is finite partition of (sub)feature Space $\mathcal{M'}$

    - induces splitting of $\{1, ..., n\}$ into $I_{D'}(M_1), ..., I_{D'}(M_p)$ (sets of indices)

    - monothetic splits: based on 1 feature

    - simple split: monothetic, into all realizations $M=\{\bar{m} \in M| m_1=a(,b...)\}$

    - binary split: monothetic, into 2 sets $M= $  $\{\bar{m} \in M| m_1\in A\}$ $\cup \{\bar{m} \in M| m_1 \notin A\}$

    - induced hypothesis $h_T(\bar{m})=T(v)$, where $v$ is unique leaf s.t. $\bar{m} \in M_v$

    - simple decision trees can represent all hypotheses

    \textbf{Decision Tree Quality Measures}
    \begin{itemize}
        \item Number of leaves
        \item Height (max number of constraints to check)
        \item External path length (sum of all path lengths from root to leaf)
        \item Weighted external path length (sum of all path lengths from root to leaf,
              weighted by number of examples classified in that leaf)
    \end{itemize}
    Theorem: Given D and bound b, its NP hard to decide existance of decision tree T s.t. $h_T$ fits D and T has ext. p.l. $\leq b$

    - Majority Class $Maj_D(M')$ maj. $r\in R$
    \\

    - Number of Misclassifications: $Err_D(M', r)$ in feature subspace $M'$ with majority class $r$

    - $Err_D(T)$: sum up all $Err_D(M_v, T(v))$

    \textbf{Pure Node} v if $Err_D(M_v, T(v))=0$

    - class distribution $p_D^{M'}(r)$: $p(r)$ in $M'$

    - \textbf{Impurity Function} $\iota: [0,1 ]^R \to \mathbb{R}$ if
    \begin{itemize}
        \item $\iota (p)$ is minimal $\forall p: p(r)=1$
        \item $\iota$ symmetric in classes
        \item $\iota$ is maximal for uniform distr.
    \end{itemize}
    gets propability distribution as input

    1. $\bar{\iota}(p)=1- \max_{r\in R}p(r)$

    2Entropy $H(p)=-\sum_{r \in R} p(r) \log_2 p(r)$

    3.Gini Impurity $G(p)=1-\sum_{r \in R} p(r)^2$

    - Impurity of $M'$ is $\iota_D(M')=\iota(p_D^{M'})$

    \textbf{Impurity Reduction} of splitting \\$\Pi=\{M'_1, ..., M'_p\}$ of $M'$ is:

    $\iota_D(\Pi)=\iota_D(M') - \sum_{i=1}^{p}\frac{|I_D(M'_i)|}{|I_D(M')|}\iota_D(M'_i)$

        \textbf{Tree Construction} for $M'\subseteq M$

        1. if no elements in $M'$: new leave $v$: $T(v)=Maj_D(M)$

        2. if $\iota (M')\leq \epsilon$: new leaf $v$: \\ $T(v)=Maj_D(M')$

        3. else: select split $\Pi$ of $M'$ with maximal impurity reduction

        - strict imp. fct.: concave at every point
        - $\iota_D(\Pi) \geq 0\ \forall \ \Pi$ and strict  imp. fct. $\iota$

        \textbf{ID3} simple D.T., monothetic simple splits, impurity function: entropy.

        inductive bias: local optimization (greedy)

        \textbf{CART} D.T., binary splits, impurity function: Gini impurity
        \\

        - true loss of $h \in H_A$: misclassifications:\\ $l^*(h)=\sum_{\bar{m}\in M}^{}(1-\delta_{h(\bar{m}), t(\bar{m})})$

        - $h$ \textbf{overfits} $D$ if $\exists h' \in H_A: $\\ $l(h, D) < l(h', D)$ and $l^*(h) > l^*(h')$

        when: training data: noisy, small, biased

        - Training Data: optimize loss here

        - Validation: optimize hyperparameters

        - Test Data: final estimation (true loss)

        - $h$ \textbf{overfits} $(D, D_V)$ if $\exists h' \in H_A: $\\ $l(h, D) < l(h', D) \wedge l(h', D_V) < l(h, D_V)$

        true loss of $h$ estimated by $l(h, D_V)$

        Countermeasures to overfitting:
        \begin{itemize}
            \item increase data quality/quantity
            \item early stopping (no more splits)

                  thrhld large -> omits useful splits

                  thrhld small -> Large Tree
            \item regularization (penalize model complexity in training process)
        \end{itemize}
        \textbf{Pruning}: turn inner node $v$ into leave with label $Maj(M_v)$

        \textbf{D.T. pruning Algorithm}

        1. given fully trained D.T.

        2. prune every inner node als long as pruning doesnt increase validation loss: $l(h_T', D_V) \leq l(h_T, D_V)$

    \newpage

\end{multicols}

\textbf{loss
    functions (and derivatives)}
\begin{itemize}
    \item $l(h, D)=\sum_{i=1}^{n}(1-\delta_{y_i, h(x_i)})$
    \item  $\delta_{ij} =1 \text{ if } i = j,0  \text{ otherwise.}$
    \item asd
\end{itemize}

\end{document}
